---
title: Open-Source LLMs
theme: ./theme/basic
layout: cover
---

# Open-Source LLMs

@yxonic & @lem0nle

---

# Contents

- What is LLM?
- Open-source LLMs
- Learn from code
  - model architecture
  - training
  - running
    - run LLaMA locally with `llama.cpp`
    - projects based on OpenAI API

---

# What is LLM?

- recent AI advances: http://dustintran.com/blog/ai-advances
- LLMs pre GPT-3: [blog](https://towardsdatascience.com/pre-trained-language-models-simplified-b8ec80c62217)
- [10 Leading Language Models For NLP In 2022](https://www.topbots.com/leading-nlp-language-models-2020/)
- [Why did all of the public reproduction of GPT-3 fail?](https://jingfengyang.github.io/gpt)
- using LLMs: [Prompt Engineering](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/)

---

# ChatGPT

- ChatGPT history review: [ChatGPT 的前世今生](https://baijiahao.baidu.com/s?id=1758688291629341140)
- [拆解追溯 GPT-3.5 各项能力的起源](https://yaofu.notion.site/GPT-3-5-360081d91ec245f29029d37b54573756)

---

# Open-Source LLMs

- [T5X](https://github.com/google-research/t5x), [Flan-T5](https://huggingface.co/google/flan-t5-base)
- [LLaMA](https://github.com/facebookresearch/llama)
- [Alpaca](https://github.com/tatsu-lab/stanford_alpaca), [website](https://crfm.stanford.edu/2023/03/13/alpaca.html)
- [Alpaca LORA](https://github.com/tloen/alpaca-lora)
- [GLM](https://github.com/THUDM/GLM)
- [ChatGLM](https://github.com/THUDM/ChatGLM-6B)
- [Dolly](https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html)
- [Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca)
- [ColossalChat](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Chat)
- [Koala](https://bair.berkeley.edu/blog/2023/04/03/koala/)
- [Vicuna](https://vicuna.lmsys.org)

---

![LLM Family](/imgs/llm_family.png)

---

# Model Architecture

- [T5](https://github.com/google-research/t5x)
- [LLaMA](https://github.com/facebookresearch/llama)
- [GLM](https://github.com/THUDM/GLM)

---

# Training

- [T5 training](https://github.com/google-research/t5x)
- [fine-tuning flan-t5](https://github.com/philschmid/deep-learning-pytorch-huggingface)

---

# SFT

- fine-tuning LLaMA: [alpaca](https://github.com/tatsu-lab/stanford_alpaca)
- [Koala](https://bair.berkeley.edu/blog/2023/04/03/koala/)
- [Vicuna](https://vicuna.lmsys.org)

---

# RLHF

- [Policy gradients and importance sampling](https://jonathan-hui.medium.com/rl-policy-gradients-explained-advanced-topic-20c2b81a9a8b)
- [PPO algorithm](https://sullivantm.com/archive/2021/09/proximal_policy_optimization/)
- [ColossalChat](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Chat)

---

# Fine-tuning efficiency

- https://www.philschmid.de/fine-tune-flan-t5-peft
- [OpenDelta](https://github.com/thunlp/OpenDelta), [news](https://mp.weixin.qq.com/s/R4pJORJ4H2DbWcHHt40YOA)

---

# Multi-modal?

- https://shikun.io/projects/prismer

---

# Efficient running

- [llama.cpp](https://github.com/ggerganov/llama.cpp)

---

# Projects based on API

- https://github.com/sigoden/aichat
- https://github.com/zurawiki/gptcommit
- https://github.com/keijiro/AICommand
