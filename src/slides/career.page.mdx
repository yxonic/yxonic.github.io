---
title: Career
theme: ./theme/basic
layout: cover
---

# Career

@yxonic

---

# 个人陈述

- 中科大陈恩红老师博士生
- 在实验室担任智慧教育方向研究组的组长
- 研究兴趣和方向主要包括：教育数据挖掘，自然语言处理，强化学习
- 在科研外有较多系统开发经历，带领开发智慧编程平台 CODIA、教育知识图谱平台 LUNA 等

---

# 个人经历：教育组

- built up a research group that explores educational data mining topics such as question understanding, student modeling, and adaptive learning and recommendation.
- mentored group members with relevant research interests; organized regular group discussion and technical report to improve communication and collaboration.
- promoted open research by setting up public website for group resources; co-leaded the team to open source educational data and model on GitHub

---

# 个人经历：CODIA

- built an intelligent online programming platform from scratch; designed and implemented the initial architecture with Node.js + Vue.
- applied user modeling and adaptive learning research from BASE group onto the platform; wrapped knowledge tracing and exercise recommendation models as micro-services.
- promoted the platform for the use of several programming and algorithm classes at USTC, providing improved learning experience for computer science students

---

# 个人经历：考试中心

- implemented a pre-trained question representation model for question auto-annotation.
- discovered novel research topics such as test paper segmentation from real-world scenarios in NEEA.
- gave talks and lectures on the principles and applications of ML/DL in educational technology.

---

# 个人经历：科大讯飞

- used NLP techniques to carry out formula extraction and similar question recommendation tasks.
- conducted question understanding and diﬀiculty estimation with deep learning methods.
- designed an exercise-enhanced knowledge tracing method for improved student performance prediction.

---

# 研究成果：Spotlight

- 任务：结构化图像转写（典型例子：公式识别为 LaTeX）
- attention 更注重内容相关，忽略图像内在结构
- spotlight 每个时刻关注一个点，以围绕中心点的高斯分布作为权重聚合
- 设计了两种策略控制聚焦中心的移动
- 引入强化学习，帮助模型学习更加合理的阅读路径
- 以现在的研究观点来看，spotlight 可以和 multi-head attention 优势互补，可以从下面几个方向改进：
  - 实现 multi-head spotlight（近期工作）
  - 和 Transformer 架构适配，去掉序列依赖，从而能够和 multi-head attention 配合使用
  - 利用局部性特征，通过定义底层计算忽略低权重位置，从而降低复杂度、提高效率

---

# 研究成果：QuesNet

- 任务：无监督预训练试题表征
- 挑战：domain-specific，多模态
- 多模态输入表示
- 通过双向 LSTM 和 self attention，分别得到 token 层和问题层表征
- 设计了双向 LSTM 上的类 BERT 预训练任务
- 引入 domain-specific task 多任务学习
- 以现在的研究观点来看，QuesNet 可以从以下角度进一步改进：
  - 通用大模型 $\to$ in-domain 大模型 $\to$ domain-specific 任务微调
  - 架构上更好地适配教育 domain 的多模态输入，特别是数学公式、几何图形等

---

# 研究成果：EKT 和 DTransformer

- 任务：知识追踪
- 目标：根据学生做题记录估计学生在每个时刻的知识掌握状态
- EKT 贡献：
  - **第一个**在 KT 中引入试题文本的工作（之前的工作均基于知识点 embedding）
  - 通过对文本 attention 聚合最相关的历史记录
  - 通过对知识矩阵 attention 形成每个知识点上的追踪
- DTransformer 贡献：
  - **第一个**使用 Transformer 诊断知识状态的工作（之前的工作只能做成绩预测）
  - 设计 DTransformer 架构，实现从问题层面到知识层面的能力诊断
  - 设计对比学习损失提升知识状态的稳定性

---

# Highlights

- 做有影响力的研究
  - Google Scholar 总引用量 772, h-index 11
- 0-1 的实现能力
  - 代码能力：多年编程经历，熟悉多种编程语言，NOI 国家奖牌，并行优化比赛、RDMA 编程比赛第一
  - 工程能力：丰富的开源项目，CODIA 和 LUNA
  - 学习能力：涉猎广泛，喜欢跟踪前沿研究和技术

---

# Q&A

---

# Reference

- [Preparation cheet sheet](https://sites.google.com/view/datascience-cheat-sheets)
- [ML interview](https://towardsdatascience.com/types-of-machine-learning-interviews-and-how-to-ace-them-51587a95f847)
- [Soft skill interview](https://towardsdatascience.com/how-to-prepare-for-a-behavioral-soft-skills-interview-cheat-sheet-9347aaeaef82)
- [ML system interview](https://towardsdatascience.com/what-is-machine-learning-system-design-interview-and-how-to-prepare-for-it-537d1271d754)
- [Resume](https://aqeel-anwar.medium.com/machine-learning-resume-that-got-me-shortlisted-for-meta-microsoft-nvidia-uber-samsung-intel-220761c1a850)
- [Resume dos and don'ts](https://aqeel-anwar.medium.com/the-dos-and-donts-of-a-software-engineer-resume-to-get-you-shortlisted-for-interviews-more-7d6a926156c0)
- [deep learning interviews](https://arxiv.org/abs/2201.00650)
